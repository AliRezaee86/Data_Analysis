{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2007099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6e483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Microsoft VS Code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01efae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\\Regional_Water_Company\\For_WR_Report\\New_Geo_Data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change to your project directory (update the path as needed)\n",
    "os.chdir(r\"H:\\Regional_Water_Company\\For_WR_Report\\New_Geo_Data\")\n",
    "\n",
    "# Verify the change\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd780883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd97ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 point_name  Depth_1  Depth_2 Type_Geo Elevation  Total_Depth  \\\n",
      "0  abasabad tokhmanjir asma      0.0      3.0       GC      1057         26.6   \n",
      "1  abasabad tokhmanjir asma      3.0      6.6       GP      1057         26.6   \n",
      "2  abasabad tokhmanjir asma      6.6      9.4       CL      1057         26.6   \n",
      "3  abasabad tokhmanjir asma      9.4     14.0       GP      1057         26.6   \n",
      "4  abasabad tokhmanjir asma     14.0     15.5       CL      1057         26.6   \n",
      "\n",
      "    Lon_D    Lat_D  Thickness_meters  Thickness_Percent  \n",
      "0  59.429  36.5557               3.0          11.278195  \n",
      "1  59.429  36.5557               3.6          13.533835  \n",
      "2  59.429  36.5557               2.8          10.526316  \n",
      "3  59.429  36.5557               4.6          17.293233  \n",
      "4  59.429  36.5557               1.5           5.639098  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "Geo_Data = pd.read_excel(\"New_Merge_Data_dd.xlsx\")\n",
    "# Show the first 5 rows\n",
    "print(Geo_Data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b4eaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geological Layer Classifications:\n",
      "\n",
      "Coarse-grained Layers:\n",
      "  GP: Poorly Graded Gravel\n",
      "  GW: Well-Graded Gravel\n",
      "  SM: Silty Sand\n",
      "  SP: Poorly Graded Sand\n",
      "  SW: Well-Graded Sand\n",
      "  SC: Clayey Sand\n",
      "\n",
      "Fine-grained Layers:\n",
      "  GC: Gravelly Clay\n",
      "  CL: Clay (Low Plasticity)\n",
      "  CL-ML: Clay-Silt Mixture\n",
      "  ML: Silt (Low Plasticity)\n",
      "  OL: Organic Silt/Clay\n",
      "  CCL: Clay (Low Plasticity)\n",
      "  CH: Clay (High Plasticity)\n",
      "  GM: Gravelly Silt\n",
      "\n",
      "Medium Layers:\n",
      "  GP-GM: Gravel-Silt Mixture\n",
      "  SC-SM: Sand-Silt Mixture\n",
      "\n",
      "Uncategorized Layers:\n",
      "  Undifind: Undefined\n",
      "  BEDROCK: Bedrock\n",
      "  <Null>: Missing Data\n",
      "\n",
      "\n",
      "Number of Unique Points per Category:\n",
      "  Coarse-grained: 381\n",
      "  Fine-grained: 446\n",
      "  Medium: 3\n",
      "  Uncategorized: 249\n",
      "\n",
      "Thickness Status Distribution (Unique Points):\n",
      "  Sum < 100 (Status -1): 25\n",
      "  Sum = 100 (Status 0): 431\n",
      "  Sum > 100 (Status 1): 18\n",
      "\n",
      "Thickness Percent Statistics by Category:\n",
      "                 count       mean        std       min       25%        50%  \\\n",
      "Category                                                                      \n",
      "Coarse-grained  1114.0  13.244300  13.501291  0.400000  4.605263   8.856209   \n",
      "Fine-grained    1645.0  15.780527  15.792146  0.403551  5.555556  10.588235   \n",
      "Medium             3.0   9.000000   4.358899  4.000000  7.500000  11.000000   \n",
      "Uncategorized    389.0  18.620216  15.518967  0.555556  7.619048  14.285714   \n",
      "\n",
      "                      75%    max  \n",
      "Category                          \n",
      "Coarse-grained  16.666667  100.0  \n",
      "Fine-grained    20.000000  100.0  \n",
      "Medium          11.500000   12.0  \n",
      "Uncategorized   25.000000  100.0  \n",
      "\n",
      "Sample of Updated DataFrame (Original):\n",
      "                 point_name Type_Geo              Full_Name        Category  \\\n",
      "0  abasabad tokhmanjir asma       GC          Gravelly Clay    Fine-grained   \n",
      "1  abasabad tokhmanjir asma       GP   Poorly Graded Gravel  Coarse-grained   \n",
      "2  abasabad tokhmanjir asma       CL  Clay (Low Plasticity)    Fine-grained   \n",
      "3  abasabad tokhmanjir asma       GP   Poorly Graded Gravel  Coarse-grained   \n",
      "4  abasabad tokhmanjir asma       CL  Clay (Low Plasticity)    Fine-grained   \n",
      "\n",
      "   Thickness_Percent  Thickness_Sum  Thickness_Difference  Thickness_Status  \n",
      "0          11.278195          100.0                   0.0                 0  \n",
      "1          13.533835          100.0                   0.0                 0  \n",
      "2          10.526316          100.0                   0.0                 0  \n",
      "3          17.293233          100.0                   0.0                 0  \n",
      "4           5.639098          100.0                   0.0                 0  \n",
      "\n",
      "Sample of Filtered DataFrame (|Thickness_Difference| <= 1):\n",
      "                 point_name Type_Geo              Full_Name        Category  \\\n",
      "0  abasabad tokhmanjir asma       GC          Gravelly Clay    Fine-grained   \n",
      "1  abasabad tokhmanjir asma       GP   Poorly Graded Gravel  Coarse-grained   \n",
      "2  abasabad tokhmanjir asma       CL  Clay (Low Plasticity)    Fine-grained   \n",
      "3  abasabad tokhmanjir asma       GP   Poorly Graded Gravel  Coarse-grained   \n",
      "4  abasabad tokhmanjir asma       CL  Clay (Low Plasticity)    Fine-grained   \n",
      "\n",
      "   Thickness_Percent  Thickness_Sum  Thickness_Difference  Thickness_Status  \n",
      "0          11.278195          100.0                   0.0                 0  \n",
      "1          13.533835          100.0                   0.0                 0  \n",
      "2          10.526316          100.0                   0.0                 0  \n",
      "3          17.293233          100.0                   0.0                 0  \n",
      "4           5.639098          100.0                   0.0                 0  \n",
      "\n",
      "Points Analysis:\n",
      "  Total unique points in original data: 474\n",
      "  Total unique points in filtered data: 458\n",
      "  Points removed (|Thickness_Difference| > 1): 16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the geological data\n",
    "Geo_Data = pd.read_excel(\"New_Merge_Data_dd.xlsx\")\n",
    "\n",
    "# Define the geological layer abbreviations, full names, and categories\n",
    "layer_definitions = {\n",
    "    'GC': {'full_name': 'Gravelly Clay', 'category': 'Fine-grained'},\n",
    "    'GP': {'full_name': 'Poorly Graded Gravel', 'category': 'Coarse-grained'},\n",
    "    'CL': {'full_name': 'Clay (Low Plasticity)', 'category': 'Fine-grained'},\n",
    "    'Undifind': {'full_name': 'Undefined', 'category': 'Uncategorized'},\n",
    "    'GW': {'full_name': 'Well-Graded Gravel', 'category': 'Coarse-grained'},\n",
    "    'SM': {'full_name': 'Silty Sand', 'category': 'Coarse-grained'},\n",
    "    'CL-ML': {'full_name': 'Clay-Silt Mixture', 'category': 'Fine-grained'},\n",
    "    'BEDROCK': {'full_name': 'Bedrock', 'category': 'Uncategorized'},\n",
    "    'SP': {'full_name': 'Poorly Graded Sand', 'category': 'Coarse-grained'},\n",
    "    'SW': {'full_name': 'Well-Graded Sand', 'category': 'Coarse-grained'},\n",
    "    'ML': {'full_name': 'Silt (Low Plasticity)', 'category': 'Fine-grained'},\n",
    "    'OL': {'full_name': 'Organic Silt/Clay', 'category': 'Fine-grained'},\n",
    "    'SC': {'full_name': 'Clayey Sand', 'category': 'Coarse-grained'},\n",
    "    'CCL': {'full_name': 'Clay (Low Plasticity)', 'category': 'Fine-grained'},\n",
    "    'CH': {'full_name': 'Clay (High Plasticity)', 'category': 'Fine-grained'},\n",
    "    'GM': {'full_name': 'Gravelly Silt', 'category': 'Fine-grained'},\n",
    "    'GP-GM': {'full_name': 'Gravel-Silt Mixture', 'category': 'Medium'},\n",
    "    'SC-SM': {'full_name': 'Sand-Silt Mixture', 'category': 'Medium'},\n",
    "    '<Null>': {'full_name': 'Missing Data', 'category': 'Uncategorized'}\n",
    "}\n",
    "\n",
    "# Function to print layer definitions by category\n",
    "def print_layer_categories():\n",
    "    print(\"Geological Layer Classifications:\\n\")\n",
    "    for category in ['Coarse-grained', 'Fine-grained', 'Medium', 'Uncategorized']:\n",
    "        print(f\"{category} Layers:\")\n",
    "        for abbr, info in layer_definitions.items():\n",
    "            if info['category'] == category:\n",
    "                print(f\"  {abbr}: {info['full_name']}\")\n",
    "        print()\n",
    "\n",
    "# Print the categorized layers\n",
    "print_layer_categories()\n",
    "\n",
    "# Add full names and categories to the DataFrame\n",
    "Geo_Data['Full_Name'] = Geo_Data['Type_Geo'].map(lambda x: layer_definitions.get(x, {'full_name': 'Unknown'})['full_name'])\n",
    "Geo_Data['Category'] = Geo_Data['Type_Geo'].map(lambda x: layer_definitions.get(x, {'category': 'Uncategorized'})['category'])\n",
    "\n",
    "# Calculate thickness sum per point and add validation columns\n",
    "Geo_Data['Thickness_Sum'] = Geo_Data.groupby('point_name')['Thickness_Percent'].transform('sum')\n",
    "Geo_Data['Thickness_Difference'] = Geo_Data['Thickness_Sum'] - 100\n",
    "Geo_Data['Thickness_Status'] = Geo_Data['Thickness_Sum'].apply(\n",
    "    lambda x: 1 if x > 100 else (-1 if x < 100 else 0)\n",
    ")\n",
    "\n",
    "# Analysis: Count unique points per category\n",
    "point_category_counts = Geo_Data.groupby('Category')['point_name'].nunique()\n",
    "print(\"\\nNumber of Unique Points per Category:\")\n",
    "for category, count in point_category_counts.items():\n",
    "    print(f\"  {category}: {count}\")\n",
    "\n",
    "# Analysis: Thickness_Status distribution\n",
    "thickness_status_counts = Geo_Data.groupby('Thickness_Status')['point_name'].nunique()\n",
    "print(\"\\nThickness Status Distribution (Unique Points):\")\n",
    "print(f\"  Sum < 100 (Status -1): {thickness_status_counts.get(-1, 0)}\")\n",
    "print(f\"  Sum = 100 (Status 0): {thickness_status_counts.get(0, 0)}\")\n",
    "print(f\"  Sum > 100 (Status 1): {thickness_status_counts.get(1, 0)}\")\n",
    "\n",
    "# Analysis: Thickness_Percent statistics by category\n",
    "thickness_stats = Geo_Data.groupby('Category')['Thickness_Percent'].describe()\n",
    "print(\"\\nThickness Percent Statistics by Category:\")\n",
    "print(thickness_stats)\n",
    "\n",
    "# Filter out points where the absolute Thickness_Difference is greater than 1\n",
    "Filtered_Geo_Data = Geo_Data[Geo_Data['Thickness_Difference'].abs() <= 1]\n",
    "filtered_points = Filtered_Geo_Data['point_name'].nunique()\n",
    "\n",
    "# Save the complete and filtered DataFrames to Excel\n",
    "Geo_Data.to_excel(\"New_Merge_Data_with_Layers.xlsx\", index=False)\n",
    "Filtered_Geo_Data.to_excel(\"New_Merge_Data_with_Layers_Filtered.xlsx\", index=False)\n",
    "\n",
    "# Display a summary of the data\n",
    "print(\"\\nSample of Updated DataFrame (Original):\")\n",
    "print(Geo_Data[['point_name', 'Type_Geo', 'Full_Name', 'Category', 'Thickness_Percent', \n",
    "                'Thickness_Sum', 'Thickness_Difference', 'Thickness_Status']].head())\n",
    "print(\"\\nSample of Filtered DataFrame (|Thickness_Difference| <= 1):\")\n",
    "print(Filtered_Geo_Data[['point_name', 'Type_Geo', 'Full_Name', 'Category', 'Thickness_Percent', \n",
    "                        'Thickness_Sum', 'Thickness_Difference', 'Thickness_Status']].head())\n",
    "\n",
    "# Analysis: Number of points filtered out\n",
    "original_points = Geo_Data['point_name'].nunique()\n",
    "print(f\"\\nPoints Analysis:\")\n",
    "print(f\"  Total unique points in original data: {original_points}\")\n",
    "print(f\"  Total unique points in filtered data: {filtered_points}\")\n",
    "print(f\"  Points removed (|Thickness_Difference| > 1): {original_points - filtered_points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63144a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geological Layer Classifications:\n",
      "\n",
      "Coarse-grained Layers:\n",
      "\n",
      "Fine-grained Layers:\n",
      "\n",
      "Medium Layers:\n",
      "  SM: Silty Sand\n",
      "  SP: Poorly Graded Sand\n",
      "  SW: Well-Graded Sand\n",
      "  SC: Clayey Sand\n",
      "  SC-SM: Sand-Silt Mixture\n",
      "\n",
      "Uncategorized Layers:\n",
      "  Undifind: Undefined\n",
      "  <Null>: Missing Data\n",
      "\n",
      "\n",
      "Number of Unique Points per Category:\n",
      "  BED: 23\n",
      "  Coarse: 412\n",
      "  Fine: 337\n",
      "  Medium: 182\n",
      "  Uncategorized: 231\n",
      "\n",
      "Thickness Status Distribution (Unique Points):\n",
      "  Sum < 100 (Status -1): 25\n",
      "  Sum = 100 (Status 0): 431\n",
      "  Sum > 100 (Status 1): 18\n",
      "\n",
      "Thickness Percent Statistics by Category:\n",
      "                count       mean        std       min       25%        50%  \\\n",
      "Category                                                                     \n",
      "BED              23.0  13.894067  10.214882  0.555556  7.035330  13.793103   \n",
      "Coarse         1244.0  14.797436  14.986441  0.400000  5.084746  10.000000   \n",
      "Fine           1206.0  15.130547  15.404152  0.403551  5.263158  10.000000   \n",
      "Medium          312.0  13.091880  12.825346  0.714286  4.545455   8.819444   \n",
      "Uncategorized   366.0  18.917214  15.755294  1.204819  7.692308  14.285714   \n",
      "\n",
      "                     75%         max  \n",
      "Category                              \n",
      "BED            15.833333   44.444444  \n",
      "Coarse         18.750000  100.000000  \n",
      "Fine           19.312312  100.000000  \n",
      "Medium         16.375969   90.555556  \n",
      "Uncategorized  25.000000  100.000000  \n",
      "\n",
      "Sample of Updated DataFrame (Original):\n",
      "                 point_name Type_Geo              Full_Name Category  \\\n",
      "0  abasabad tokhmanjir asma       GC          Gravelly Clay   Coarse   \n",
      "1  abasabad tokhmanjir asma       GP   Poorly Graded Gravel   Coarse   \n",
      "2  abasabad tokhmanjir asma       CL  Clay (Low Plasticity)     Fine   \n",
      "3  abasabad tokhmanjir asma       GP   Poorly Graded Gravel   Coarse   \n",
      "4  abasabad tokhmanjir asma       CL  Clay (Low Plasticity)     Fine   \n",
      "\n",
      "   Thickness_Percent  Thickness_Sum  Thickness_Difference  Thickness_Status  \n",
      "0          11.278195          100.0                   0.0                 0  \n",
      "1          13.533835          100.0                   0.0                 0  \n",
      "2          10.526316          100.0                   0.0                 0  \n",
      "3          17.293233          100.0                   0.0                 0  \n",
      "4           5.639098          100.0                   0.0                 0  \n",
      "\n",
      "Sample of Filtered DataFrame (|Thickness_Difference| <= 1):\n",
      "                 point_name Type_Geo              Full_Name Category  \\\n",
      "0  abasabad tokhmanjir asma       GC          Gravelly Clay   Coarse   \n",
      "1  abasabad tokhmanjir asma       GP   Poorly Graded Gravel   Coarse   \n",
      "2  abasabad tokhmanjir asma       CL  Clay (Low Plasticity)     Fine   \n",
      "3  abasabad tokhmanjir asma       GP   Poorly Graded Gravel   Coarse   \n",
      "4  abasabad tokhmanjir asma       CL  Clay (Low Plasticity)     Fine   \n",
      "\n",
      "   Thickness_Percent  Thickness_Sum  Thickness_Difference  Thickness_Status  \n",
      "0          11.278195          100.0                   0.0                 0  \n",
      "1          13.533835          100.0                   0.0                 0  \n",
      "2          10.526316          100.0                   0.0                 0  \n",
      "3          17.293233          100.0                   0.0                 0  \n",
      "4           5.639098          100.0                   0.0                 0  \n",
      "\n",
      "Points Analysis:\n",
      "  Total unique points in original data: 474\n",
      "  Total unique points in filtered data: 458\n",
      "  Points removed (|Thickness_Difference| > 1): 16\n"
     ]
    }
   ],
   "source": [
    "## To classify the given soil types into three categories: Fine, Medium, and Coarse, along with Uncategorized\n",
    "## To classify the given soil types into three categories: Fine, Medium, and Coarse, along with Uncategorized\n",
    "## To classify the given soil types into three categories: Fine, Medium, and Coarse, along with Uncategorized\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the geological data\n",
    "Geo_Data = pd.read_excel(\"New_Merge_Data_dd.xlsx\")\n",
    "\n",
    "# Define the geological layer abbreviations, full names, and categories\n",
    "layer_definitions = {\n",
    "    'GC': {'full_name': 'Gravelly Clay', 'category': 'Coarse'},\n",
    "    'GP': {'full_name': 'Poorly Graded Gravel', 'category': 'Coarse'},\n",
    "    'CL': {'full_name': 'Clay (Low Plasticity)', 'category': 'Fine'},\n",
    "    'Undifind': {'full_name': 'Undefined', 'category': 'Uncategorized'},\n",
    "    'GW': {'full_name': 'Well-Graded Gravel', 'category': 'Coarse'},\n",
    "    'SM': {'full_name': 'Silty Sand', 'category': 'Medium'},\n",
    "    'CL-ML': {'full_name': 'Clay-Silt Mixture', 'category': 'Fine'},\n",
    "    'BEDROCK': {'full_name': 'Bedrock', 'category': 'BED'},\n",
    "    'SP': {'full_name': 'Poorly Graded Sand', 'category': 'Medium'},\n",
    "    'SW': {'full_name': 'Well-Graded Sand', 'category': 'Medium'},\n",
    "    'ML': {'full_name': 'Silt (Low Plasticity)', 'category': 'Fine'},\n",
    "    'OL': {'full_name': 'Organic Silt/Clay', 'category': 'Fine'},\n",
    "    'SC': {'full_name': 'Clayey Sand', 'category': 'Medium'},\n",
    "    'CCL': {'full_name': 'Clay (Low Plasticity)', 'category': 'Fine'},\n",
    "    'CH': {'full_name': 'Clay (High Plasticity)', 'category': 'Fine'},\n",
    "    'GM': {'full_name': 'Gravelly Silt', 'category': 'Coarse'},\n",
    "    'GP-GM': {'full_name': 'Gravel-Silt Mixture', 'category': 'Coarse'},\n",
    "    'SC-SM': {'full_name': 'Sand-Silt Mixture', 'category': 'Medium'},\n",
    "    '<Null>': {'full_name': 'Missing Data', 'category': 'Uncategorized'}\n",
    "}\n",
    "\n",
    "# Function to print layer definitions by category\n",
    "def print_layer_categories():\n",
    "    print(\"Geological Layer Classifications:\\n\")\n",
    "    for category in ['Coarse-grained', 'Fine-grained', 'Medium', 'Uncategorized']:\n",
    "        print(f\"{category} Layers:\")\n",
    "        for abbr, info in layer_definitions.items():\n",
    "            if info['category'] == category:\n",
    "                print(f\"  {abbr}: {info['full_name']}\")\n",
    "        print()\n",
    "\n",
    "# Print the categorized layers\n",
    "print_layer_categories()\n",
    "\n",
    "# Add full names and categories to the DataFrame\n",
    "Geo_Data['Full_Name'] = Geo_Data['Type_Geo'].map(lambda x: layer_definitions.get(x, {'full_name': 'Unknown'})['full_name'])\n",
    "Geo_Data['Category'] = Geo_Data['Type_Geo'].map(lambda x: layer_definitions.get(x, {'category': 'Uncategorized'})['category'])\n",
    "\n",
    "# Calculate thickness sum per point and add validation columns\n",
    "Geo_Data['Thickness_Sum'] = Geo_Data.groupby('point_name')['Thickness_Percent'].transform('sum')\n",
    "Geo_Data['Thickness_Difference'] = Geo_Data['Thickness_Sum'] - 100\n",
    "Geo_Data['Thickness_Status'] = Geo_Data['Thickness_Sum'].apply(\n",
    "    lambda x: 1 if x > 100 else (-1 if x < 100 else 0)\n",
    ")\n",
    "\n",
    "# Analysis: Count unique points per category\n",
    "point_category_counts = Geo_Data.groupby('Category')['point_name'].nunique()\n",
    "print(\"\\nNumber of Unique Points per Category:\")\n",
    "for category, count in point_category_counts.items():\n",
    "    print(f\"  {category}: {count}\")\n",
    "\n",
    "# Analysis: Thickness_Status distribution\n",
    "thickness_status_counts = Geo_Data.groupby('Thickness_Status')['point_name'].nunique()\n",
    "print(\"\\nThickness Status Distribution (Unique Points):\")\n",
    "print(f\"  Sum < 100 (Status -1): {thickness_status_counts.get(-1, 0)}\")\n",
    "print(f\"  Sum = 100 (Status 0): {thickness_status_counts.get(0, 0)}\")\n",
    "print(f\"  Sum > 100 (Status 1): {thickness_status_counts.get(1, 0)}\")\n",
    "\n",
    "# Analysis: Thickness_Percent statistics by category\n",
    "thickness_stats = Geo_Data.groupby('Category')['Thickness_Percent'].describe()\n",
    "print(\"\\nThickness Percent Statistics by Category:\")\n",
    "print(thickness_stats)\n",
    "\n",
    "# Filter out points where the absolute Thickness_Difference is greater than 1\n",
    "Filtered_Geo_Data = Geo_Data[Geo_Data['Thickness_Difference'].abs() <= 1]\n",
    "filtered_points = Filtered_Geo_Data['point_name'].nunique()\n",
    "\n",
    "# Save the complete and filtered DataFrames to Excel\n",
    "Geo_Data.to_excel(\"New_Merge_Data_with_Layers_4Categories.xlsx\", index=False)\n",
    "Filtered_Geo_Data.to_excel(\"New_Merge_Data_with_Layers_Filtered_4Categories.xlsx\", index=False)\n",
    "\n",
    "# Display a summary of the data\n",
    "print(\"\\nSample of Updated DataFrame (Original):\")\n",
    "print(Geo_Data[['point_name', 'Type_Geo', 'Full_Name', 'Category', 'Thickness_Percent', \n",
    "                'Thickness_Sum', 'Thickness_Difference', 'Thickness_Status']].head())\n",
    "print(\"\\nSample of Filtered DataFrame (|Thickness_Difference| <= 1):\")\n",
    "print(Filtered_Geo_Data[['point_name', 'Type_Geo', 'Full_Name', 'Category', 'Thickness_Percent', \n",
    "                        'Thickness_Sum', 'Thickness_Difference', 'Thickness_Status']].head())\n",
    "\n",
    "# Analysis: Number of points filtered out\n",
    "original_points = Geo_Data['point_name'].nunique()\n",
    "print(f\"\\nPoints Analysis:\")\n",
    "print(f\"  Total unique points in original data: {original_points}\")\n",
    "print(f\"  Total unique points in filtered data: {filtered_points}\")\n",
    "print(f\"  Points removed (|Thickness_Difference| > 1): {original_points - filtered_points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f6c783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Excel file saved to: with_Water_Levels_1992_2024.xlsx\n",
      "\n",
      "Sample of Updated DataFrame with Water Levels:\n",
      "                 point_name   Lon_D    Lat_D     WL2024     WL1992\n",
      "0  abasabad tokhmanjir asma  59.429  36.5557  72.269569  42.360306\n",
      "1  abasabad tokhmanjir asma  59.429  36.5557  72.269569  42.360306\n",
      "2  abasabad tokhmanjir asma  59.429  36.5557  72.269569  42.360306\n",
      "3  abasabad tokhmanjir asma  59.429  36.5557  72.269569  42.360306\n",
      "4  abasabad tokhmanjir asma  59.429  36.5557  72.269569  42.360306\n",
      "\n",
      "Rows with Invalid or Out-of-Bounds Coordinates:\n",
      "Row 1383: Point = joeh paien4 asma, Lon = <Null>, Lat = <Null>, Issue = Invalid or missing coordinates\n",
      "Row 1384: Point = joeh paien4 asma, Lon = <Null>, Lat = <Null>, Issue = Invalid or missing coordinates\n",
      "Row 1385: Point = joeh paien4 asma, Lon = <Null>, Lat = <Null>, Issue = Invalid or missing coordinates\n",
      "Row 1386: Point = joeh paien4 asma, Lon = <Null>, Lat = <Null>, Issue = Invalid or missing coordinates\n",
      "Row 1387: Point = joeh paien4 asma, Lon = <Null>, Lat = <Null>, Issue = Invalid or missing coordinates\n",
      "Row 1388: Point = joeh paien4 asma, Lon = <Null>, Lat = <Null>, Issue = Invalid or missing coordinates\n",
      "Row 1389: Point = joeh paien4 asma, Lon = <Null>, Lat = <Null>, Issue = Invalid or missing coordinates\n",
      "Row 1390: Point = joeh paien4 asma, Lon = <Null>, Lat = <Null>, Issue = Invalid or missing coordinates\n",
      "Row 1444: Point = kalateh bolorian1, Lon = 58.633, Lat = 36.9809, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1444: Point = kalateh bolorian1, Lon = 58.633, Lat = 36.9809, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1445: Point = kalateh bolorian1, Lon = 58.633, Lat = 36.9809, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1445: Point = kalateh bolorian1, Lon = 58.633, Lat = 36.9809, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1446: Point = kalateh bolorian1, Lon = 58.633, Lat = 36.9809, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1446: Point = kalateh bolorian1, Lon = 58.633, Lat = 36.9809, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1465: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1465: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1466: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1466: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1467: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1467: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1468: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1468: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1469: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1469: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1470: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1470: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1471: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1471: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1472: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1472: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1473: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1473: Point = kalateh milan, Lon = 59.155, Lat = 37.5765, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1745: Point = langarak asma, Lon = 59.951, Lat = 36.1932, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1745: Point = langarak asma, Lon = 59.951, Lat = 36.1932, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1746: Point = langarak asma, Lon = 59.951, Lat = 36.1932, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1746: Point = langarak asma, Lon = 59.951, Lat = 36.1932, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1747: Point = langarak asma, Lon = 59.951, Lat = 36.1932, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1747: Point = langarak asma, Lon = 59.951, Lat = 36.1932, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1748: Point = langarak asma, Lon = 59.951, Lat = 36.1932, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1748: Point = langarak asma, Lon = 59.951, Lat = 36.1932, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1749: Point = langarak asma, Lon = 59.951, Lat = 36.1932, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1749: Point = langarak asma, Lon = 59.951, Lat = 36.1932, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 1757: Point = mahmoodabad kharabeh1 asma, Lon = 59.159, Lat = 3.6367, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 1757: Point = mahmoodabad kharabeh1 asma, Lon = 59.159, Lat = 3.6367, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 3001: Point = yasaghi1, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 3001: Point = yasaghi1, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 3002: Point = yasaghi1, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 3002: Point = yasaghi1, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 3003: Point = yasaghi1, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 3003: Point = yasaghi1, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 3004: Point = yasaghi2, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 3004: Point = yasaghi2, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 3005: Point = yasaghi2, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 3005: Point = yasaghi2, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 3006: Point = yasaghi2, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 3006: Point = yasaghi2, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 3007: Point = yasaghi2, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 3007: Point = yasaghi2, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 1992 raster bounds\n",
      "Row 3008: Point = yasaghi2, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 2024 raster bounds\n",
      "Row 3008: Point = yasaghi2, Lon = 58.699, Lat = 36.9707, Issue = Coordinates outside 1992 raster bounds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import rowcol\n",
    "import numpy as np\n",
    "\n",
    "# File paths\n",
    "tiff_path_2024 = \"WL2024.tif\"\n",
    "tiff_path_1992 = \"WL1992.tif\"\n",
    "excel_path = \"New_Merge_Data_with_Layers_Filtered_4Categories.xlsx\"\n",
    "output_excel_path = \"with_Water_Levels_1992_2024.xlsx\"\n",
    "\n",
    "try:\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(excel_path)\n",
    "\n",
    "    # Initialize lists for water levels and problematic rows\n",
    "    water_levels_2024 = []\n",
    "    water_levels_1992 = []\n",
    "    problematic_rows = []\n",
    "\n",
    "    # Open the 2024 GeoTIFF file\n",
    "    with rasterio.open(tiff_path_2024) as src_2024, rasterio.open(tiff_path_1992) as src_1992:\n",
    "        # Get transforms and data\n",
    "        transform_2024 = src_2024.transform\n",
    "        raster_data_2024 = src_2024.read(1)\n",
    "        nodata_2024 = src_2024.nodata if src_2024.nodata is not None else np.nan\n",
    "\n",
    "        transform_1992 = src_1992.transform\n",
    "        raster_data_1992 = src_1992.read(1)\n",
    "        nodata_1992 = src_1992.nodata if src_1992.nodata is not None else np.nan\n",
    "\n",
    "        # Iterate through each row\n",
    "        for idx, (lon, lat, point_name) in enumerate(zip(df['Lon_D'], df['Lat_D'], df['point_name'])):\n",
    "            # Check for invalid or missing coordinates\n",
    "            if pd.isna(lon) or pd.isna(lat) or str(lon).lower() == '<null>' or str(lat).lower() == '<null>':\n",
    "                problematic_rows.append((idx, point_name, lon, lat, 'Invalid or missing coordinates'))\n",
    "                water_levels_2024.append(np.nan)\n",
    "                water_levels_1992.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Convert to float and validate\n",
    "                lon = float(lon)\n",
    "                lat = float(lat)\n",
    "\n",
    "                # Process 2024 data\n",
    "                row_2024, col_2024 = rowcol(transform_2024, lon, lat)\n",
    "                if 0 <= row_2024 < raster_data_2024.shape[0] and 0 <= col_2024 < raster_data_2024.shape[1]:\n",
    "                    value_2024 = raster_data_2024[row_2024, col_2024]\n",
    "                    water_levels_2024.append(np.nan if value_2024 == nodata_2024 or np.isnan(value_2024) else value_2024)\n",
    "                else:\n",
    "                    water_levels_2024.append(np.nan)\n",
    "                    problematic_rows.append((idx, point_name, lon, lat, 'Coordinates outside 2024 raster bounds'))\n",
    "\n",
    "                # Process 1992 data\n",
    "                row_1992, col_1992 = rowcol(transform_1992, lon, lat)\n",
    "                if 0 <= row_1992 < raster_data_1992.shape[0] and 0 <= col_1992 < raster_data_1992.shape[1]:\n",
    "                    value_1992 = raster_data_1992[row_1992, col_1992]\n",
    "                    water_levels_1992.append(np.nan if value_1992 == nodata_1992 or np.isnan(value_1992) else value_1992)\n",
    "                else:\n",
    "                    water_levels_1992.append(np.nan)\n",
    "                    problematic_rows.append((idx, point_name, lon, lat, 'Coordinates outside 1992 raster bounds'))\n",
    "\n",
    "            except (ValueError, TypeError) as e:\n",
    "                problematic_rows.append((idx, point_name, lon, lat, f'Error processing coordinates: {str(e)}'))\n",
    "                water_levels_2024.append(np.nan)\n",
    "                water_levels_1992.append(np.nan)\n",
    "\n",
    "    # Add water level columns to the DataFrame\n",
    "    df['WL2024'] = water_levels_2024\n",
    "    df['WL1992'] = water_levels_1992\n",
    "\n",
    "    # Save the updated DataFrame\n",
    "    df.to_excel(output_excel_path, index=False)\n",
    "    print(f\"Updated Excel file saved to: {output_excel_path}\")\n",
    "\n",
    "    # Display a sample of the updated DataFrame\n",
    "    print(\"\\nSample of Updated DataFrame with Water Levels:\")\n",
    "    print(df[['point_name', 'Lon_D', 'Lat_D', 'WL2024', 'WL1992']].head())\n",
    "\n",
    "    # Report problematic rows\n",
    "    if problematic_rows:\n",
    "        print(\"\\nRows with Invalid or Out-of-Bounds Coordinates:\")\n",
    "        for idx, point_name, lon, lat, issue in problematic_rows:\n",
    "            print(f\"Row {idx}: Point = {point_name}, Lon = {lon}, Lat = {lat}, Issue = {issue}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: File not found. Check paths: {tiff_path_2024}, {tiff_path_1992}, {excel_path}. Details: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Failed to process files. Details: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3b85f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Excel file saved to: Water_Level_1992_2024_Overlap.xlsx\n",
      "Original number of rows: 3035\n",
      "Filtered number of rows: 906\n",
      "\n",
      "Sample of Filtered DataFrame (Layers Overlapping WL1992 to WL2024):\n",
      "           point_name  Depth_1  Depth_2 Type_Geo     WL1992     WL2024\n",
      "8       abaskhan asma      5.0     10.0       CL   9.874941  23.229906\n",
      "9       abaskhan asma     10.0     20.0       GW   9.874941  23.229906\n",
      "10      abaskhan asma     20.0     25.0       CL   9.874941  23.229906\n",
      "15  abfaakharnakhrisi     22.0     70.0       GC  47.912453  58.952065\n",
      "20       abfaandishe3     60.0     90.0       GC  69.350441  87.593704\n",
      "\n",
      "Rows with Invalid or Missing Water Level Data:\n",
      "Row 1375: Point = joeh paien3 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1376: Point = joeh paien3 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1377: Point = joeh paien3 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1378: Point = joeh paien3 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1379: Point = joeh paien3 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1380: Point = joeh paien3 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1381: Point = joeh paien3 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1382: Point = joeh paien3 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1383: Point = joeh paien4 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1384: Point = joeh paien4 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1385: Point = joeh paien4 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1386: Point = joeh paien4 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1387: Point = joeh paien4 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1388: Point = joeh paien4 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1389: Point = joeh paien4 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1390: Point = joeh paien4 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1444: Point = kalateh bolorian1, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1445: Point = kalateh bolorian1, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1446: Point = kalateh bolorian1, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1465: Point = kalateh milan, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1466: Point = kalateh milan, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1467: Point = kalateh milan, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1468: Point = kalateh milan, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1469: Point = kalateh milan, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1470: Point = kalateh milan, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1471: Point = kalateh milan, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1472: Point = kalateh milan, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1473: Point = kalateh milan, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1717: Point = kondkhab asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1718: Point = kondkhab asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1719: Point = kondkhab asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1720: Point = kukakula asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1721: Point = kukakula asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1722: Point = kukakula asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1723: Point = kukakula asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1724: Point = kukakula asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1725: Point = kukakula asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1726: Point = kukakula asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1727: Point = kukakula asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1728: Point = kukakula asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1736: Point = kuye afsaran asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1737: Point = kuye afsaran asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1738: Point = kuye afsaran asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1739: Point = kuye afsaran asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1740: Point = kuye afsaran asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1741: Point = kuye afsaran asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1742: Point = kuye afsaran asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1743: Point = kuye afsaran asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1744: Point = kuye afsaran asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1745: Point = langarak asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1746: Point = langarak asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1747: Point = langarak asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1748: Point = langarak asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1749: Point = langarak asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1757: Point = mahmoodabad kharabeh1 asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1800: Point = manzel abad2, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1801: Point = manzel abad2, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1802: Point = manzel abad2, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 1803: Point = manzel abad2, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2083: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2084: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2085: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2086: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2087: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2088: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2089: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2090: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2091: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2092: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2093: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2094: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2095: Point = par kand abad518284, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2340: Point = shaghesh asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2341: Point = shaghesh asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2342: Point = shaghesh asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2343: Point = shaghesh asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2344: Point = shaghesh asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2345: Point = shaghesh asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2346: Point = shaghesh asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2347: Point = shaghesh asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2348: Point = shaghesh asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2349: Point = shaghesh asma, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2469: Point = tasfieh khain arab-BH3, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2470: Point = tasfieh khain arab-BH3, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2471: Point = tasfieh khain arab-BH3, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2472: Point = tasfieh khain arab-BH3, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2473: Point = tasfieh khain arab-BH3, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2474: Point = tasfieh khain arab-BH3, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2475: Point = tasfieh khain arab-BH3, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2476: Point = tasfieh khain arab-BH3, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2477: Point = tasfieh khain arab-BH3, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2875: Point = WNamayeshgah, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2876: Point = WNamayeshgah, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2942: Point = WSamenSepah, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2943: Point = WSamenSepah, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2944: Point = WSamenSepah, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2976: Point = WShirShotorAbedin, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2977: Point = WShirShotorAbedin, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2978: Point = WShirShotorAbedin, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 2979: Point = WShirShotorAbedin, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 3001: Point = yasaghi1, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 3002: Point = yasaghi1, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 3003: Point = yasaghi1, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 3004: Point = yasaghi2, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 3005: Point = yasaghi2, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 3006: Point = yasaghi2, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 3007: Point = yasaghi2, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n",
      "Row 3008: Point = yasaghi2, WL1992 = nan, WL2024 = nan, Issue = Invalid or missing water level data\n"
     ]
    }
   ],
   "source": [
    "### for two water depth 1992 to 2024 exactly in this distance 1922 and 2024\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# File paths\n",
    "input_excel_path = \"with_Water_Levels_1992_2024.xlsx\"\n",
    "output_excel_path = \"Water_Level_1992_2024_Overlap.xlsx\"\n",
    "\n",
    "try:\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(input_excel_path)\n",
    "\n",
    "    # Initialize a list to store indices of rows to keep and problematic rows\n",
    "    rows_to_keep = []\n",
    "    problematic_rows = []\n",
    "\n",
    "    # Filter rows where the layer depth range overlaps with the water level range\n",
    "    for idx, row in df.iterrows():\n",
    "        wl_2024 = row['WL2024']\n",
    "        wl_1992 = row['WL1992']\n",
    "        depth_1 = row['Depth_1']\n",
    "        depth_2 = row['Depth_2']\n",
    "        point_name = row['point_name']\n",
    "\n",
    "        # Check if water levels are valid (not NaN or <Null>)\n",
    "        if (pd.isna(wl_2024) or str(wl_2024).lower() == '<null>' or \n",
    "            pd.isna(wl_1992) or str(wl_1992).lower() == '<null>'):\n",
    "            problematic_rows.append((idx, point_name, wl_1992, wl_2024, 'Invalid or missing water level data'))\n",
    "            continue\n",
    "\n",
    "        # Calculate min and max water levels\n",
    "        min_wl = min(wl_2024, wl_1992)\n",
    "        max_wl = max(wl_2024, wl_1992)\n",
    "\n",
    "        # Keep row if layer depth range overlaps with water level range\n",
    "        if depth_1 <= max_wl and depth_2 >= min_wl:\n",
    "            rows_to_keep.append(idx)\n",
    "\n",
    "    # Create filtered DataFrame\n",
    "    filtered_df = df.loc[rows_to_keep].copy()\n",
    "\n",
    "    # Save the filtered DataFrame to a new Excel file\n",
    "    filtered_df.to_excel(output_excel_path, index=False)\n",
    "    print(f\"Filtered Excel file saved to: {output_excel_path}\")\n",
    "    print(f\"Original number of rows: {len(df)}\")\n",
    "    print(f\"Filtered number of rows: {len(filtered_df)}\")\n",
    "\n",
    "    # Display a sample of the filtered DataFrame\n",
    "    print(\"\\nSample of Filtered DataFrame (Layers Overlapping WL1992 to WL2024):\")\n",
    "    print(filtered_df[['point_name', 'Depth_1', 'Depth_2', 'Type_Geo', 'WL1992', 'WL2024']].head())\n",
    "\n",
    "    # Report problematic rows\n",
    "    if problematic_rows:\n",
    "        print(\"\\nRows with Invalid or Missing Water Level Data:\")\n",
    "        for idx, point_name, wl_1992, wl_2024, issue in problematic_rows:\n",
    "            print(f\"Row {idx}: Point = {point_name}, WL1992 = {wl_1992}, WL2024 = {wl_2024}, Issue = {issue}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: File not found. Check path: {input_excel_path}. Details: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Failed to process file. Details: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "018a3729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AAAli\\AppData\\Local\\Temp\\ipykernel_4584\\6781253.py:48: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  updated_df = grouped.apply(update_point_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Excel file saved to: Data_Updated_Depths_Total_Depth.xlsx\n",
      "Original number of rows: 906\n",
      "Updated number of rows: 906\n",
      "\n",
      "Sample of Updated DataFrame:\n",
      "      point_name  Depth_1  Depth_2 Type_Geo  Total_Depth  Thickness_meters  \\\n",
      "0  AbfaBazarReza     37.0     75.0   CL- ML    54.741631              38.0   \n",
      "1  AbfaChenaran1     30.0     40.0       SM    59.647648              10.0   \n",
      "2  AbfaChenaran1     40.0     50.0   CL- ML    59.647648              10.0   \n",
      "3  AbfaChenaran1     50.0     55.0       SM    59.647648               5.0   \n",
      "4  AbfaChenaran1     55.0     65.0       GW    59.647648              10.0   \n",
      "\n",
      "   Thickness_Percent  Water_Level_Depth  \n",
      "0          69.417004          54.741631  \n",
      "1          16.765120          59.647648  \n",
      "2          16.765120          59.647648  \n",
      "3           8.382560          59.647648  \n",
      "4          16.765120          59.647648  \n"
     ]
    }
   ],
   "source": [
    "# Replace the Total_Depth column for each point with the corresponding Water_Level_Depth value\n",
    "# Replace the Total_Depth column for each point with the corresponding Water_Level_Depth value\n",
    "# Replace the Total_Depth column for each point with the corresponding Water_Level_Depth value\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# File paths\n",
    "input_excel_path = \"Water_Level_1992_2024_Overlap.xlsx\"\n",
    "output_excel_path = \"Data_Updated_Depths_Total_Depth.xlsx\"\n",
    "\n",
    "try:\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(input_excel_path)\n",
    "\n",
    "    # Initialize lists for problematic rows\n",
    "    problematic_rows = []\n",
    "\n",
    "    # Group by point_name to ensure consistent Total_Depth per point\n",
    "    grouped = df.groupby('point_name')\n",
    "\n",
    "    # Function to update Total_Depth and recalculate Thickness columns\n",
    "    def update_point_group(group):\n",
    "        # Get the first valid Water_Level_Depth (should be same for all rows in group)\n",
    "        water_level = group['Water_Level_Depth'].dropna()\n",
    "        if water_level.empty or str(water_level.iloc[0]).lower() == '<null>':\n",
    "            problematic_rows.append((group.name, 'Missing or invalid Water_Level_Depth'))\n",
    "            return None  # Skip group with no valid water level\n",
    "\n",
    "        water_level = float(water_level.iloc[0])\n",
    "        if water_level == 0:\n",
    "            problematic_rows.append((group.name, 'Water_Level_Depth is zero'))\n",
    "            return None  # Skip to avoid division by zero\n",
    "\n",
    "        # Update Total_Depth\n",
    "        group['Total_Depth'] = water_level\n",
    "\n",
    "        # Recalculate Thickness_meters\n",
    "        group['Thickness_meters'] = group['Depth_2'] - group['Depth_1']\n",
    "\n",
    "        # Recalculate Thickness_Percent\n",
    "        group['Thickness_Percent'] = (group['Thickness_meters'] / group['Total_Depth']) * 100\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Apply updates to each group\n",
    "    updated_df = grouped.apply(update_point_group).reset_index(drop=True)\n",
    "\n",
    "    # Remove any None groups (those with invalid water levels)\n",
    "    if updated_df is not None:\n",
    "        updated_df = updated_df.dropna(subset=['Total_Depth'])\n",
    "\n",
    "    # Save the updated DataFrame\n",
    "    updated_df.to_excel(output_excel_path, index=False)\n",
    "    print(f\"Updated Excel file saved to: {output_excel_path}\")\n",
    "    print(f\"Original number of rows: {len(df)}\")\n",
    "    print(f\"Updated number of rows: {len(updated_df)}\")\n",
    "\n",
    "    # Display a sample of the updated DataFrame\n",
    "    print(\"\\nSample of Updated DataFrame:\")\n",
    "    print(updated_df[['point_name', 'Depth_1', 'Depth_2', 'Type_Geo', 'Total_Depth', \n",
    "                     'Thickness_meters', 'Thickness_Percent', 'Water_Level_Depth']].head())\n",
    "\n",
    "    # Report problematic points\n",
    "    if problematic_rows:\n",
    "        print(\"\\nProblematic Points:\")\n",
    "        for point_name, issue in problematic_rows:\n",
    "            print(f\"Point: {point_name}, Issue: {issue}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: File not found. Check path: {input_excel_path}. Details: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Failed to process file. Details: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bfa5041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_name</th>\n",
       "      <th>Depth_1</th>\n",
       "      <th>Depth_2</th>\n",
       "      <th>Type_Geo</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Total_Depth</th>\n",
       "      <th>Lon_D</th>\n",
       "      <th>Lat_D</th>\n",
       "      <th>Thickness_meters</th>\n",
       "      <th>Thickness_Percent</th>\n",
       "      <th>Full_Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Thickness_Sum</th>\n",
       "      <th>Thickness_Difference</th>\n",
       "      <th>Thickness_Status</th>\n",
       "      <th>Water_Level_Depth</th>\n",
       "      <th>WL1992</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abaskhan asma</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>CL</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>59.228</td>\n",
       "      <td>36.6651</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>Clay (Low Plasticity)</td>\n",
       "      <td>Fine</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.229906</td>\n",
       "      <td>9.874941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abaskhan asma</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>GW</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>59.228</td>\n",
       "      <td>36.6651</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>Well-Graded Gravel</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.229906</td>\n",
       "      <td>9.874941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abaskhan asma</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>CL</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>59.228</td>\n",
       "      <td>36.6651</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>Clay (Low Plasticity)</td>\n",
       "      <td>Fine</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.229906</td>\n",
       "      <td>9.874941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abfaakharnakhrisi</td>\n",
       "      <td>22.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>GC</td>\n",
       "      <td>980.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>59.619</td>\n",
       "      <td>36.2625</td>\n",
       "      <td>48.0</td>\n",
       "      <td>25.531915</td>\n",
       "      <td>Gravelly Clay</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.952065</td>\n",
       "      <td>47.912453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abfaandishe3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>GC</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>59.516</td>\n",
       "      <td>36.3807</td>\n",
       "      <td>30.0</td>\n",
       "      <td>13.888889</td>\n",
       "      <td>Gravelly Clay</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>87.593704</td>\n",
       "      <td>69.350441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>zanaghel asma</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>CL</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>59.159</td>\n",
       "      <td>36.6447</td>\n",
       "      <td>14.0</td>\n",
       "      <td>21.875000</td>\n",
       "      <td>Clay (Low Plasticity)</td>\n",
       "      <td>Fine</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>25.860218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>zanaghel asma</td>\n",
       "      <td>39.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>GP</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>59.159</td>\n",
       "      <td>36.6447</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>Poorly Graded Gravel</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>25.860218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>zanaghel asma</td>\n",
       "      <td>41.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>CL</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>59.159</td>\n",
       "      <td>36.6447</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>Clay (Low Plasticity)</td>\n",
       "      <td>Fine</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>25.860218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>zanaghel asma</td>\n",
       "      <td>45.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>GW</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>59.159</td>\n",
       "      <td>36.6447</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>Well-Graded Gravel</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>25.860218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>zanaghel asma</td>\n",
       "      <td>46.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>SP</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>59.159</td>\n",
       "      <td>36.6447</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>Poorly Graded Sand</td>\n",
       "      <td>Medium</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>25.860218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>906 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            point_name  Depth_1  Depth_2 Type_Geo  Elevation  Total_Depth  \\\n",
       "0        abaskhan asma      5.0     10.0       CL     1170.0         90.0   \n",
       "1        abaskhan asma     10.0     20.0       GW     1170.0         90.0   \n",
       "2        abaskhan asma     20.0     25.0       CL     1170.0         90.0   \n",
       "3    abfaakharnakhrisi     22.0     70.0       GC      980.0        188.0   \n",
       "4         abfaandishe3     60.0     90.0       GC     1019.0        216.0   \n",
       "..                 ...      ...      ...      ...        ...          ...   \n",
       "901      zanaghel asma     25.0     39.0       CL     1158.0         64.0   \n",
       "902      zanaghel asma     39.0     41.0       GP     1158.0         64.0   \n",
       "903      zanaghel asma     41.0     45.0       CL     1158.0         64.0   \n",
       "904      zanaghel asma     45.0     46.0       GW     1158.0         64.0   \n",
       "905      zanaghel asma     46.0     58.0       SP     1158.0         64.0   \n",
       "\n",
       "      Lon_D    Lat_D  Thickness_meters  Thickness_Percent  \\\n",
       "0    59.228  36.6651               5.0           5.555556   \n",
       "1    59.228  36.6651              10.0          11.111111   \n",
       "2    59.228  36.6651               5.0           5.555556   \n",
       "3    59.619  36.2625              48.0          25.531915   \n",
       "4    59.516  36.3807              30.0          13.888889   \n",
       "..      ...      ...               ...                ...   \n",
       "901  59.159  36.6447              14.0          21.875000   \n",
       "902  59.159  36.6447               2.0           3.125000   \n",
       "903  59.159  36.6447               4.0           6.250000   \n",
       "904  59.159  36.6447               1.0           1.562500   \n",
       "905  59.159  36.6447              12.0          18.750000   \n",
       "\n",
       "                 Full_Name Category  Thickness_Sum  Thickness_Difference  \\\n",
       "0    Clay (Low Plasticity)     Fine          100.0                   0.0   \n",
       "1       Well-Graded Gravel   Coarse          100.0                   0.0   \n",
       "2    Clay (Low Plasticity)     Fine          100.0                   0.0   \n",
       "3            Gravelly Clay   Coarse          100.0                   0.0   \n",
       "4            Gravelly Clay   Coarse          100.0                   0.0   \n",
       "..                     ...      ...            ...                   ...   \n",
       "901  Clay (Low Plasticity)     Fine          100.0                   0.0   \n",
       "902   Poorly Graded Gravel   Coarse          100.0                   0.0   \n",
       "903  Clay (Low Plasticity)     Fine          100.0                   0.0   \n",
       "904     Well-Graded Gravel   Coarse          100.0                   0.0   \n",
       "905     Poorly Graded Sand   Medium          100.0                   0.0   \n",
       "\n",
       "     Thickness_Status  Water_Level_Depth     WL1992  \n",
       "0                   0          23.229906   9.874941  \n",
       "1                   0          23.229906   9.874941  \n",
       "2                   0          23.229906   9.874941  \n",
       "3                   0          58.952065  47.912453  \n",
       "4                   0          87.593704  69.350441  \n",
       "..                ...                ...        ...  \n",
       "901                 0          46.127895  25.860218  \n",
       "902                 0          46.127895  25.860218  \n",
       "903                 0          46.127895  25.860218  \n",
       "904                 0          46.127895  25.860218  \n",
       "905                 0          46.127895  25.860218  \n",
       "\n",
       "[906 rows x 17 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_excel_path = \"Water_Level_1992_2024_Overlap.xlsx\"\n",
    "df = pd.read_excel(input_excel_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f959d53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'updated_data.csv'\n",
      "Columns in the saved dataset: ['point_name', 'Depth_1', 'Depth_2', 'Type_Geo', 'Elevation', 'Total_Depth', 'Lon_D', 'Lat_D', 'Thickness_meters', 'Thickness_Percent', 'Full_Name', 'Category', 'Thickness_Sum', 'Thickness_Difference', 'Thickness_Status', 'Water_Level_Depth', 'WL1992']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "input_excel_path = \"Water_Level_1992_2024_Overlap.xlsx\"\n",
    "df = pd.read_excel(input_excel_path)\n",
    "df\n",
    "# Step 1: Sort by point_name and Depth_2 to ensure the last row is the deepest for each point\n",
    "df = df.sort_values(by=['point_name', 'Depth_2'])\n",
    "\n",
    "# Step 2: Update Depth_2 for the last row of each point_name with Water_Level_Depth\n",
    "last_rows = df.groupby('point_name').tail(1).index\n",
    "df.loc[last_rows, 'Depth_2'] = df.loc[last_rows, 'Water_Level_Depth']\n",
    "\n",
    "# Step 3: Update Total_Depth for each point_name with Water_Level_Depth\n",
    "df['Total_Depth'] = df.groupby('point_name')['Water_Level_Depth'].transform('first')\n",
    "\n",
    "# Step 4: Sort back to original order to maintain the original row sequence\n",
    "df = df.sort_index()\n",
    "\n",
    "# Step 5: Save the updated DataFrame with all columns to a CSV file\n",
    "df.to_csv('updated_data.csv', index=False)\n",
    "\n",
    "# Step 6: Confirm the data has been saved and show the columns preserved\n",
    "print(\"Data has been saved to 'updated_data.csv'\")\n",
    "print(\"Columns in the saved dataset:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dfe5984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_name</th>\n",
       "      <th>Depth_1</th>\n",
       "      <th>Depth_2</th>\n",
       "      <th>Type_Geo</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Total_Depth</th>\n",
       "      <th>Lon_D</th>\n",
       "      <th>Lat_D</th>\n",
       "      <th>Thickness_meters</th>\n",
       "      <th>Thickness_Percent</th>\n",
       "      <th>Full_Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Thickness_Sum</th>\n",
       "      <th>Thickness_Difference</th>\n",
       "      <th>Thickness_Status</th>\n",
       "      <th>Water_Level_Depth</th>\n",
       "      <th>WL1992</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abaskhan asma</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>CL</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>23.229906</td>\n",
       "      <td>59.228</td>\n",
       "      <td>36.6651</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>Clay (Low Plasticity)</td>\n",
       "      <td>Fine</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.229906</td>\n",
       "      <td>9.874941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abaskhan asma</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>GW</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>23.229906</td>\n",
       "      <td>59.228</td>\n",
       "      <td>36.6651</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>Well-Graded Gravel</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.229906</td>\n",
       "      <td>9.874941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abaskhan asma</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.229906</td>\n",
       "      <td>CL</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>23.229906</td>\n",
       "      <td>59.228</td>\n",
       "      <td>36.6651</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>Clay (Low Plasticity)</td>\n",
       "      <td>Fine</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.229906</td>\n",
       "      <td>9.874941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abfaakharnakhrisi</td>\n",
       "      <td>22.0</td>\n",
       "      <td>58.952065</td>\n",
       "      <td>GC</td>\n",
       "      <td>980.0</td>\n",
       "      <td>58.952065</td>\n",
       "      <td>59.619</td>\n",
       "      <td>36.2625</td>\n",
       "      <td>48.0</td>\n",
       "      <td>25.531915</td>\n",
       "      <td>Gravelly Clay</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.952065</td>\n",
       "      <td>47.912453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abfaandishe3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>87.593704</td>\n",
       "      <td>GC</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>87.593704</td>\n",
       "      <td>59.516</td>\n",
       "      <td>36.3807</td>\n",
       "      <td>30.0</td>\n",
       "      <td>13.888889</td>\n",
       "      <td>Gravelly Clay</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>87.593704</td>\n",
       "      <td>69.350441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>zanaghel asma</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>CL</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>59.159</td>\n",
       "      <td>36.6447</td>\n",
       "      <td>14.0</td>\n",
       "      <td>21.875000</td>\n",
       "      <td>Clay (Low Plasticity)</td>\n",
       "      <td>Fine</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>25.860218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>zanaghel asma</td>\n",
       "      <td>39.0</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>GP</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>59.159</td>\n",
       "      <td>36.6447</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>Poorly Graded Gravel</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>25.860218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>zanaghel asma</td>\n",
       "      <td>41.0</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>CL</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>59.159</td>\n",
       "      <td>36.6447</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>Clay (Low Plasticity)</td>\n",
       "      <td>Fine</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>25.860218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>zanaghel asma</td>\n",
       "      <td>45.0</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>GW</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>59.159</td>\n",
       "      <td>36.6447</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>Well-Graded Gravel</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>25.860218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>zanaghel asma</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>SP</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>59.159</td>\n",
       "      <td>36.6447</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>Poorly Graded Sand</td>\n",
       "      <td>Medium</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.127895</td>\n",
       "      <td>25.860218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>906 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            point_name  Depth_1    Depth_2 Type_Geo  Elevation  Total_Depth  \\\n",
       "0        abaskhan asma      5.0  10.000000       CL     1170.0    23.229906   \n",
       "1        abaskhan asma     10.0  20.000000       GW     1170.0    23.229906   \n",
       "2        abaskhan asma     20.0  23.229906       CL     1170.0    23.229906   \n",
       "3    abfaakharnakhrisi     22.0  58.952065       GC      980.0    58.952065   \n",
       "4         abfaandishe3     60.0  87.593704       GC     1019.0    87.593704   \n",
       "..                 ...      ...        ...      ...        ...          ...   \n",
       "901      zanaghel asma     25.0  39.000000       CL     1158.0    46.127895   \n",
       "902      zanaghel asma     39.0  41.000000       GP     1158.0    46.127895   \n",
       "903      zanaghel asma     41.0  45.000000       CL     1158.0    46.127895   \n",
       "904      zanaghel asma     45.0  46.000000       GW     1158.0    46.127895   \n",
       "905      zanaghel asma     46.0  46.127895       SP     1158.0    46.127895   \n",
       "\n",
       "      Lon_D    Lat_D  Thickness_meters  Thickness_Percent  \\\n",
       "0    59.228  36.6651               5.0           5.555556   \n",
       "1    59.228  36.6651              10.0          11.111111   \n",
       "2    59.228  36.6651               5.0           5.555556   \n",
       "3    59.619  36.2625              48.0          25.531915   \n",
       "4    59.516  36.3807              30.0          13.888889   \n",
       "..      ...      ...               ...                ...   \n",
       "901  59.159  36.6447              14.0          21.875000   \n",
       "902  59.159  36.6447               2.0           3.125000   \n",
       "903  59.159  36.6447               4.0           6.250000   \n",
       "904  59.159  36.6447               1.0           1.562500   \n",
       "905  59.159  36.6447              12.0          18.750000   \n",
       "\n",
       "                 Full_Name Category  Thickness_Sum  Thickness_Difference  \\\n",
       "0    Clay (Low Plasticity)     Fine          100.0                   0.0   \n",
       "1       Well-Graded Gravel   Coarse          100.0                   0.0   \n",
       "2    Clay (Low Plasticity)     Fine          100.0                   0.0   \n",
       "3            Gravelly Clay   Coarse          100.0                   0.0   \n",
       "4            Gravelly Clay   Coarse          100.0                   0.0   \n",
       "..                     ...      ...            ...                   ...   \n",
       "901  Clay (Low Plasticity)     Fine          100.0                   0.0   \n",
       "902   Poorly Graded Gravel   Coarse          100.0                   0.0   \n",
       "903  Clay (Low Plasticity)     Fine          100.0                   0.0   \n",
       "904     Well-Graded Gravel   Coarse          100.0                   0.0   \n",
       "905     Poorly Graded Sand   Medium          100.0                   0.0   \n",
       "\n",
       "     Thickness_Status  Water_Level_Depth     WL1992  \n",
       "0                   0          23.229906   9.874941  \n",
       "1                   0          23.229906   9.874941  \n",
       "2                   0          23.229906   9.874941  \n",
       "3                   0          58.952065  47.912453  \n",
       "4                   0          87.593704  69.350441  \n",
       "..                ...                ...        ...  \n",
       "901                 0          46.127895  25.860218  \n",
       "902                 0          46.127895  25.860218  \n",
       "903                 0          46.127895  25.860218  \n",
       "904                 0          46.127895  25.860218  \n",
       "905                 0          46.127895  25.860218  \n",
       "\n",
       "[906 rows x 17 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4b2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_excel_path = \"Water_Level_1992_2024_Overlap.xlsx\"\n",
    "df = pd.read_excel(input_excel_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3006994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated main dataset saved to 'updated_data.csv'\n",
      "Percentage of fine, medium, and coarse layers saved to 'fine_percentage.csv'\n",
      "Total thickness of fine layers saved to 'fine_thickness.csv'\n",
      "\n",
      "Columns in updated dataset: ['point_name', 'Depth_1', 'Depth_2', 'Type_Geo', 'Elevation', 'Total_Depth', 'Lon_D', 'Lat_D', 'Thickness_meters', 'Thickness_Percent', 'Full_Name', 'Category', 'Thickness_Sum', 'Thickness_Difference', 'Thickness_Status', 'Water_Level_Depth', 'WL1992']\n",
      "\n",
      "Sample of updated main dataset:\n",
      "          point_name  Depth_1    Depth_2  Total_Depth  Thickness_meters  \\\n",
      "0      abaskhan asma      5.0  10.000000    23.229906          5.000000   \n",
      "1      abaskhan asma     10.0  20.000000    23.229906         10.000000   \n",
      "2      abaskhan asma     20.0  23.229906    23.229906          3.229906   \n",
      "3  abfaakharnakhrisi     22.0  58.952065    58.952065         36.952065   \n",
      "4       abfaandishe3     60.0  87.593704    87.593704         27.593704   \n",
      "5     abfaazadshahr2      0.0  65.000000    72.720261         65.000000   \n",
      "6     abfaazadshahr2     65.0  72.720261    72.720261          7.720261   \n",
      "7      AbfaBazarReza     37.0  54.741631    54.741631         17.741631   \n",
      "8       abfachamran4      0.0  25.000000    33.098812         25.000000   \n",
      "9       abfachamran4     25.0  33.098812    33.098812          8.098812   \n",
      "\n",
      "   Thickness_Percent  Water_Level_Depth  \n",
      "0          21.523979          23.229906  \n",
      "1          43.047957          23.229906  \n",
      "2          13.904086          23.229906  \n",
      "3          62.681544          58.952065  \n",
      "4          31.501926          87.593704  \n",
      "5          89.383618          72.720261  \n",
      "6          10.616382          72.720261  \n",
      "7          32.409759          54.741631  \n",
      "8          75.531412          33.098812  \n",
      "9          24.468588          33.098812  \n",
      "\n",
      "Sample of fine_percentage.csv:\n",
      "Category         point_name  BED     Coarse  Fine     Medium  Uncategorized\n",
      "0             AbfaBazarReza  0.0   0.000000   0.0   0.000000      32.409759\n",
      "1             AbfaChenaran1  0.0   7.791838   0.0  25.147681      16.765120\n",
      "2             AbfaChenaran2  0.0   0.000000   0.0  62.566207       0.000000\n",
      "3             AbfaChenaran3  0.0  70.833686   0.0  29.166314       0.000000\n",
      "4         AbfaChenaranGihan  0.0  79.130048   0.0   0.000000       0.000000\n",
      "\n",
      "Sample of fine_thickness.csv:\n",
      "      point_name  Fine_Thickness_meters\n",
      "0       WDehbagh               2.877995\n",
      "1  WGholGhoochan              30.000000\n",
      "2  abaskhan asma               8.229906\n",
      "3    abkuh2 asma              15.247437\n",
      "4    abkuh3 asma               6.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset from updated_data.csv\n",
    "df = pd.read_csv('updated_data.csv')\n",
    "\n",
    "# Step 2: Recalculate Thickness_meters as Depth_2 - Depth_1\n",
    "df['Thickness_meters'] = df['Depth_2'] - df['Depth_1']\n",
    "\n",
    "# Step 3: Recalculate Thickness_Percent as (Thickness_meters / Total_Depth) * 100\n",
    "df['Thickness_Percent'] = (df['Thickness_meters'] / df['Total_Depth']) * 100\n",
    "\n",
    "# Step 4: Calculate percentage of fine, medium, and coarse layers for each point_name\n",
    "# Group by point_name and Category, sum Thickness_meters\n",
    "thickness_by_category = df.groupby(['point_name', 'Category'])['Thickness_meters'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Calculate percentage for each category relative to Total_Depth\n",
    "# Get Total_Depth for each point_name (it's the same for all rows of a point_name)\n",
    "total_depth = df.groupby('point_name')['Total_Depth'].first()\n",
    "percentage_by_category = thickness_by_category.div(total_depth, axis=0) * 100\n",
    "percentage_by_category = percentage_by_category.fillna(0).reset_index()\n",
    "\n",
    "# Step 5: Calculate total thickness of fine layers in meters\n",
    "fine_thickness = df[df['Category'] == 'Fine'].groupby('point_name')['Thickness_meters'].sum().reset_index()\n",
    "fine_thickness.columns = ['point_name', 'Fine_Thickness_meters']\n",
    "\n",
    "# Step 6: Save the percentage of fine, medium, and coarse layers to a CSV file\n",
    "percentage_by_category.to_csv('fine_percentage.csv', index=False)\n",
    "\n",
    "# Step 7: Save the total thickness of fine layers to a CSV file\n",
    "fine_thickness.to_csv('fine_thickness.csv', index=False)\n",
    "\n",
    "# Step 8: Save the updated main dataset with all 17 columns\n",
    "df.to_csv('updated_data_final.csv', index=False)\n",
    "\n",
    "# Step 9: Confirm outputs and show sample data\n",
    "print(\"Updated main dataset saved to 'updated_data.csv'\")\n",
    "print(\"Percentage of fine, medium, and coarse layers saved to 'fine_percentage.csv'\")\n",
    "print(\"Total thickness of fine layers saved to 'fine_thickness.csv'\")\n",
    "print(\"\\nColumns in updated dataset:\", df.columns.tolist())\n",
    "print(\"\\nSample of updated main dataset:\")\n",
    "print(df[['point_name', 'Depth_1', 'Depth_2', 'Total_Depth', 'Thickness_meters', 'Thickness_Percent', 'Water_Level_Depth']].head(10))\n",
    "print(\"\\nSample of fine_percentage.csv:\")\n",
    "print(percentage_by_category.head())\n",
    "print(\"\\nSample of fine_thickness.csv:\")\n",
    "print(fine_thickness.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0b681ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated main dataset saved to 'updated_data.csv'\n",
      "Percentage of fine, medium, and coarse layers saved to 'fine_percentage.csv'\n",
      "Total thickness of fine layers saved to 'fine_thickness.csv'\n",
      "\n",
      "Columns in updated dataset: ['point_name', 'Depth_1', 'Depth_2', 'Type_Geo', 'Elevation', 'Total_Depth', 'Lon_D', 'Lat_D', 'Thickness_meters', 'Thickness_Percent', 'Full_Name', 'Category', 'Thickness_Sum', 'Thickness_Difference', 'Thickness_Status', 'Water_Level_Depth', 'WL1992']\n",
      "\n",
      "Sample of updated main dataset:\n",
      "          point_name  Depth_1    Depth_2  Total_Depth  Thickness_meters  \\\n",
      "0      abaskhan asma      5.0  10.000000    23.229906          5.000000   \n",
      "1      abaskhan asma     10.0  20.000000    23.229906         10.000000   \n",
      "2      abaskhan asma     20.0  23.229906    23.229906          3.229906   \n",
      "3  abfaakharnakhrisi     22.0  58.952065    58.952065         36.952065   \n",
      "4       abfaandishe3     60.0  87.593704    87.593704         27.593704   \n",
      "5     abfaazadshahr2      0.0  65.000000    72.720261         65.000000   \n",
      "6     abfaazadshahr2     65.0  72.720261    72.720261          7.720261   \n",
      "7      AbfaBazarReza     37.0  54.741631    54.741631         17.741631   \n",
      "8       abfachamran4      0.0  25.000000    33.098812         25.000000   \n",
      "9       abfachamran4     25.0  33.098812    33.098812          8.098812   \n",
      "\n",
      "   Thickness_Percent  Water_Level_Depth  \n",
      "0          21.523979          23.229906  \n",
      "1          43.047957          23.229906  \n",
      "2          13.904086          23.229906  \n",
      "3          62.681544          58.952065  \n",
      "4          31.501926          87.593704  \n",
      "5          89.383618          72.720261  \n",
      "6          10.616382          72.720261  \n",
      "7          32.409759          54.741631  \n",
      "8          75.531412          33.098812  \n",
      "9          24.468588          33.098812  \n",
      "\n",
      "Sample of fine_percentage.csv:\n",
      "          point_name  Fine     Medium     Coarse  Fine_Thickness_meters  \\\n",
      "0      AbfaBazarReza   0.0   0.000000   0.000000                    0.0   \n",
      "1      AbfaChenaran1   0.0  25.147681   7.791838                    0.0   \n",
      "2      AbfaChenaran2   0.0  62.566207   0.000000                    0.0   \n",
      "3      AbfaChenaran3   0.0  29.166314  70.833686                    0.0   \n",
      "4  AbfaChenaranGihan   0.0   0.000000  79.130048                    0.0   \n",
      "\n",
      "   Fine_Percentage  Elevation  Total_Depth    Lat_D   Lon_D  \n",
      "0              0.0      975.0    54.741631  36.2763  59.615  \n",
      "1              0.0     1192.0    59.647648  36.6301  59.105  \n",
      "2              0.0     1180.0    66.784576  36.8193  58.919  \n",
      "3              0.0     1192.0    68.572258  36.6323  59.116  \n",
      "4              0.0     1209.0    95.831558  36.6144  59.128  \n",
      "\n",
      "Sample of fine_thickness.csv:\n",
      "      point_name  Fine_Thickness_meters  Fine_Percentage  Elevation  \\\n",
      "0       WDehbagh               2.877995         5.888119     1164.0   \n",
      "1  WGholGhoochan              30.000000        60.519820     1170.0   \n",
      "2  abaskhan asma               8.229906        35.428064     1170.0   \n",
      "3    abkuh2 asma              15.247437        23.368637      997.0   \n",
      "4    abkuh3 asma               6.000000         9.236188     1000.0   \n",
      "\n",
      "   Total_Depth    Lat_D   Lon_D  \n",
      "0    48.877995  36.6903  59.069  \n",
      "1    49.570538  36.6629  59.087  \n",
      "2    23.229906  36.6651  59.228  \n",
      "3    65.247437  36.3386  59.566  \n",
      "4    64.961868  36.3373  59.564  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset from updated_data.csv\n",
    "df = pd.read_csv('updated_data.csv')\n",
    "\n",
    "# Step 2: Recalculate Thickness_meters as Depth_2 - Depth_1\n",
    "df['Thickness_meters'] = df['Depth_2'] - df['Depth_1']\n",
    "\n",
    "# Step 3: Recalculate Thickness_Percent as (Thickness_meters / Total_Depth) * 100\n",
    "df['Thickness_Percent'] = (df['Thickness_meters'] / df['Total_Depth']) * 100\n",
    "\n",
    "# Step 4: Calculate percentage of fine, medium, and coarse layers for each point_name\n",
    "# Group by point_name and Category, sum Thickness_meters\n",
    "thickness_by_category = df.groupby(['point_name', 'Category'])['Thickness_meters'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Calculate percentage for each category relative to Total_Depth\n",
    "# Get Total_Depth for each point_name (it's the same for all rows of a point_name)\n",
    "total_depth = df.groupby('point_name')['Total_Depth'].first()\n",
    "percentage_by_category = thickness_by_category.div(total_depth, axis=0) * 100\n",
    "percentage_by_category = percentage_by_category.fillna(0).reset_index()\n",
    "\n",
    "# Step 5: Calculate total thickness of fine layers in meters\n",
    "fine_thickness = df[df['Category'] == 'Fine'].groupby('point_name')['Thickness_meters'].sum().reset_index()\n",
    "fine_thickness.columns = ['point_name', 'Fine_Thickness_meters']\n",
    "\n",
    "# Step 6: Get additional columns (Elevation, Lat_D, Lon_D) for each point_name\n",
    "# Since these are constant per point_name, take the first value\n",
    "additional_info = df.groupby('point_name').agg({\n",
    "    'Elevation': 'first',\n",
    "    'Total_Depth': 'first',\n",
    "    'Lat_D': 'first',\n",
    "    'Lon_D': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate Fine_Percentage for merging\n",
    "fine_percentage = percentage_by_category[['point_name', 'Fine']].rename(columns={'Fine': 'Fine_Percentage'})\n",
    "\n",
    "# Step 7: Merge additional columns into fine_percentage and fine_thickness\n",
    "# Merge with percentage_by_category\n",
    "percentage_by_category = percentage_by_category.merge(additional_info, on='point_name', how='left')\n",
    "percentage_by_category = percentage_by_category.merge(fine_thickness, on='point_name', how='left')\n",
    "percentage_by_category['Fine_Thickness_meters'] = percentage_by_category['Fine_Thickness_meters'].fillna(0)\n",
    "percentage_by_category = percentage_by_category.merge(fine_percentage, on='point_name', how='left')\n",
    "\n",
    "# Merge with fine_thickness\n",
    "fine_thickness = fine_thickness.merge(additional_info, on='point_name', how='left')\n",
    "fine_thickness = fine_thickness.merge(fine_percentage, on='point_name', how='left')\n",
    "\n",
    "# Reorder columns for fine_percentage.csv\n",
    "fine_percentage_cols = ['point_name', 'Fine', 'Medium', 'Coarse', 'Fine_Thickness_meters', 'Fine_Percentage', 'Elevation', 'Total_Depth', 'Lat_D', 'Lon_D']\n",
    "percentage_by_category = percentage_by_category[fine_percentage_cols]\n",
    "\n",
    "# Reorder columns for fine_thickness.csv\n",
    "fine_thickness_cols = ['point_name', 'Fine_Thickness_meters', 'Fine_Percentage', 'Elevation', 'Total_Depth', 'Lat_D', 'Lon_D']\n",
    "fine_thickness = fine_thickness[fine_thickness_cols]\n",
    "\n",
    "# Step 8: Save the percentage of fine, medium, and coarse layers to a CSV file\n",
    "percentage_by_category.to_csv('fine_percentage.csv', index=False)\n",
    "\n",
    "# Step 9: Save the total thickness of fine layers to a CSV file\n",
    "fine_thickness.to_csv('fine_thickness.csv', index=False)\n",
    "\n",
    "# Step 10: Save the updated main dataset with all 17 columns\n",
    "df.to_csv('updated_data_final.csv', index=False)\n",
    "\n",
    "# Step 11: Confirm outputs and show sample data\n",
    "print(\"Updated main dataset saved to 'updated_data.csv'\")\n",
    "print(\"Percentage of fine, medium, and coarse layers saved to 'fine_percentage.csv'\")\n",
    "print(\"Total thickness of fine layers saved to 'fine_thickness.csv'\")\n",
    "print(\"\\nColumns in updated dataset:\", df.columns.tolist())\n",
    "print(\"\\nSample of updated main dataset:\")\n",
    "print(df[['point_name', 'Depth_1', 'Depth_2', 'Total_Depth', 'Thickness_meters', 'Thickness_Percent', 'Water_Level_Depth']].head(10))\n",
    "print(\"\\nSample of fine_percentage.csv:\")\n",
    "print(percentage_by_category.head())\n",
    "print(\"\\nSample of fine_thickness.csv:\")\n",
    "print(fine_thickness.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b57c9dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated main dataset saved to 'updated_data_final.csv'\n",
      "Percentage of fine, medium, and coarse layers saved to 'fine_percentage.csv'\n",
      "Total thickness of fine layers saved to 'fine_thickness.csv'\n",
      "Bar chart saved to 'layer_percentage_bar.png'\n",
      "Pie charts saved as 'pie_chart_<point_name>.png' for each point\n",
      "Histogram of Fine percentages saved to 'fine_percentage_histogram.png'\n",
      "Scatter plot of Fine Thickness vs Total Depth saved to 'fine_thickness_vs_total_depth.png'\n",
      "\n",
      "Columns in updated dataset: ['point_name', 'Depth_1', 'Depth_2', 'Type_Geo', 'Elevation', 'Total_Depth', 'Lon_D', 'Lat_D', 'Thickness_meters', 'Thickness_Percent', 'Full_Name', 'Category', 'Thickness_Sum', 'Thickness_Difference', 'Thickness_Status', 'Water_Level_Depth', 'WL1992']\n",
      "\n",
      "Sample of updated main dataset:\n",
      "          point_name  Depth_1    Depth_2  Total_Depth  Thickness_meters  \\\n",
      "0      abaskhan asma      5.0  10.000000    23.229906          5.000000   \n",
      "1      abaskhan asma     10.0  20.000000    23.229906         10.000000   \n",
      "2      abaskhan asma     20.0  23.229906    23.229906          3.229906   \n",
      "3  abfaakharnakhrisi     22.0  58.952065    58.952065         36.952065   \n",
      "4       abfaandishe3     60.0  87.593704    87.593704         27.593704   \n",
      "5     abfaazadshahr2      0.0  65.000000    72.720261         65.000000   \n",
      "6     abfaazadshahr2     65.0  72.720261    72.720261          7.720261   \n",
      "7      AbfaBazarReza     37.0  54.741631    54.741631         17.741631   \n",
      "8       abfachamran4      0.0  25.000000    33.098812         25.000000   \n",
      "9       abfachamran4     25.0  33.098812    33.098812          8.098812   \n",
      "\n",
      "   Thickness_Percent  Water_Level_Depth  \n",
      "0          21.523979          23.229906  \n",
      "1          43.047957          23.229906  \n",
      "2          13.904086          23.229906  \n",
      "3          62.681544          58.952065  \n",
      "4          31.501926          87.593704  \n",
      "5          89.383618          72.720261  \n",
      "6          10.616382          72.720261  \n",
      "7          32.409759          54.741631  \n",
      "8          75.531412          33.098812  \n",
      "9          24.468588          33.098812  \n",
      "\n",
      "Sample of fine_percentage.csv:\n",
      "          point_name  Fine     Medium     Coarse  Fine_Thickness_meters  \\\n",
      "0      AbfaBazarReza   0.0   0.000000   0.000000                    0.0   \n",
      "1      AbfaChenaran1   0.0  25.147681   7.791838                    0.0   \n",
      "2      AbfaChenaran2   0.0  62.566207   0.000000                    0.0   \n",
      "3      AbfaChenaran3   0.0  29.166314  70.833686                    0.0   \n",
      "4  AbfaChenaranGihan   0.0   0.000000  79.130048                    0.0   \n",
      "\n",
      "   Fine_Percentage  Elevation  Total_Depth    Lat_D   Lon_D  \n",
      "0              0.0      975.0    54.741631  36.2763  59.615  \n",
      "1              0.0     1192.0    59.647648  36.6301  59.105  \n",
      "2              0.0     1180.0    66.784576  36.8193  58.919  \n",
      "3              0.0     1192.0    68.572258  36.6323  59.116  \n",
      "4              0.0     1209.0    95.831558  36.6144  59.128  \n",
      "\n",
      "Sample of fine_thickness.csv:\n",
      "      point_name  Fine_Thickness_meters  Fine_Percentage  Elevation  \\\n",
      "0       WDehbagh               2.877995         5.888119     1164.0   \n",
      "1  WGholGhoochan              30.000000        60.519820     1170.0   \n",
      "2  abaskhan asma               8.229906        35.428064     1170.0   \n",
      "3    abkuh2 asma              15.247437        23.368637      997.0   \n",
      "4    abkuh3 asma               6.000000         9.236188     1000.0   \n",
      "\n",
      "   Total_Depth    Lat_D   Lon_D  \n",
      "0    48.877995  36.6903  59.069  \n",
      "1    49.570538  36.6629  59.087  \n",
      "2    23.229906  36.6651  59.228  \n",
      "3    65.247437  36.3386  59.566  \n",
      "4    64.961868  36.3373  59.564  \n",
      "\n",
      "Summary statistics for Fine, Medium, Coarse percentages:\n",
      "            Fine      Medium      Coarse\n",
      "mean   18.035780    4.549658   26.922455\n",
      "50%     4.541019    0.000000   18.516274\n",
      "min     0.000000    0.000000    0.000000\n",
      "max   100.000000  100.000000  100.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Step 1: Load the dataset from updated_data.csv\n",
    "df = pd.read_csv('updated_data.csv')\n",
    "\n",
    "# Step 2: Recalculate Thickness_meters as Depth_2 - Depth_1\n",
    "df['Thickness_meters'] = df['Depth_2'] - df['Depth_1']\n",
    "\n",
    "# Step 3: Recalculate Thickness_Percent as (Thickness_meters / Total_Depth) * 100\n",
    "df['Thickness_Percent'] = (df['Thickness_meters'] / df['Total_Depth']) * 100\n",
    "\n",
    "# Step 4: Calculate percentage of fine, medium, and coarse layers for each point_name\n",
    "# Group by point_name and Category, sum Thickness_meters\n",
    "thickness_by_category = df.groupby(['point_name', 'Category'])['Thickness_meters'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Calculate percentage for each category relative to Total_Depth\n",
    "# Get Total_Depth for each point_name (it's the same for all rows of a point_name)\n",
    "total_depth = df.groupby('point_name')['Total_Depth'].first()\n",
    "percentage_by_category = thickness_by_category.div(total_depth, axis=0) * 100\n",
    "percentage_by_category = percentage_by_category.fillna(0).reset_index()\n",
    "\n",
    "# Step 5: Calculate total thickness of fine layers in meters\n",
    "fine_thickness = df[df['Category'] == 'Fine'].groupby('point_name')['Thickness_meters'].sum().reset_index()\n",
    "fine_thickness.columns = ['point_name', 'Fine_Thickness_meters']\n",
    "\n",
    "# Step 6: Get additional columns (Elevation, Lat_D, Lon_D) for each point_name\n",
    "# Since these are constant per point_name, take the first value\n",
    "additional_info = df.groupby('point_name').agg({\n",
    "    'Elevation': 'first',\n",
    "    'Total_Depth': 'first',\n",
    "    'Lat_D': 'first',\n",
    "    'Lon_D': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate Fine_Percentage for merging\n",
    "fine_percentage = percentage_by_category[['point_name', 'Fine']].rename(columns={'Fine': 'Fine_Percentage'})\n",
    "\n",
    "# Step 7: Merge additional columns into fine_percentage and fine_thickness\n",
    "# Merge with percentage_by_category\n",
    "percentage_by_category = percentage_by_category.merge(additional_info, on='point_name', how='left')\n",
    "percentage_by_category = percentage_by_category.merge(fine_thickness, on='point_name', how='left')\n",
    "percentage_by_category['Fine_Thickness_meters'] = percentage_by_category['Fine_Thickness_meters'].fillna(0)\n",
    "percentage_by_category = percentage_by_category.merge(fine_percentage, on='point_name', how='left')\n",
    "\n",
    "# Merge with fine_thickness\n",
    "fine_thickness = fine_thickness.merge(additional_info, on='point_name', how='left')\n",
    "fine_thickness = fine_thickness.merge(fine_percentage, on='point_name', how='left')\n",
    "\n",
    "# Reorder columns for fine_percentage.csv\n",
    "fine_percentage_cols = ['point_name', 'Fine', 'Medium', 'Coarse', 'Fine_Thickness_meters', 'Fine_Percentage', 'Elevation', 'Total_Depth', 'Lat_D', 'Lon_D']\n",
    "percentage_by_category = percentage_by_category[fine_percentage_cols]\n",
    "\n",
    "# Reorder columns for fine_thickness.csv\n",
    "fine_thickness_cols = ['point_name', 'Fine_Thickness_meters', 'Fine_Percentage', 'Elevation', 'Total_Depth', 'Lat_D', 'Lon_D']\n",
    "fine_thickness = fine_thickness[fine_thickness_cols]\n",
    "\n",
    "# Step 8: Save the percentage of fine, medium, and coarse layers to a CSV file\n",
    "#percentage_by_category.to_csv('fine_percentage.csv', index=False)\n",
    "\n",
    "# Step 9: Save the total thickness of fine layers to a CSV file\n",
    "#fine_thickness.to_csv('fine_thickness.csv', index=False)\n",
    "\n",
    "# Step 10: Save the updated main dataset with all 17 columns\n",
    "#df.to_csv('updated_data_final.csv', index=False)\n",
    "\n",
    "# Step 11: Analyze results with visualizations\n",
    "# Bar chart for Fine, Medium, Coarse percentages\n",
    "plt.figure(figsize=(12, 6))\n",
    "percentage_by_category.set_index('point_name')[['Fine', 'Medium', 'Coarse']].plot(kind='bar', stacked=True, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "plt.title('Percentage of Fine, Medium, and Coarse Layers by Point')\n",
    "plt.xlabel('Point Name')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.legend(title='Category')\n",
    "plt.tight_layout()\n",
    "plt.savefig('layer_percentage_bar.png')\n",
    "plt.close()\n",
    "\n",
    "# Pie charts for each point_name\n",
    "for point in percentage_by_category['point_name']:\n",
    "    point_data = percentage_by_category[percentage_by_category['point_name'] == point][['Fine', 'Medium', 'Coarse']].iloc[0]\n",
    "    labels = ['Fine', 'Medium', 'Coarse']\n",
    "    sizes = [point_data['Fine'], point_data['Medium'], point_data['Coarse']]\n",
    "    # Only include non-zero values in the pie chart\n",
    "    non_zero_labels = [label for label, size in zip(labels, sizes) if size > 0]\n",
    "    non_zero_sizes = [size for size in sizes if size > 0]\n",
    "    if non_zero_sizes:  # Only create pie chart if there are non-zero values\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.pie(non_zero_sizes, labels=non_zero_labels, autopct='%1.1f%%', colors=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "        plt.title(f'Layer Distribution for {point}')\n",
    "        plt.tight_layout()\n",
    "        # Replace invalid characters in point_name for filename\n",
    "        safe_point_name = point.replace(' ', '_').replace('/', '_').replace('\\\\', '_')\n",
    "        plt.savefig(f'pie_chart_{safe_point_name}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Histogram of Fine percentages\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(percentage_by_category['Fine'], bins=20, color='#1f77b4', kde=True)\n",
    "plt.title('Distribution of Fine Layer Percentages')\n",
    "plt.xlabel('Fine Percentage (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fine_percentage_histogram.png')\n",
    "plt.close()\n",
    "\n",
    "# Scatter plot of Fine_Thickness_meters vs Total_Depth with Elevation as color\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(percentage_by_category['Total_Depth'], percentage_by_category['Fine_Thickness_meters'], \n",
    "                     c=percentage_by_category['Elevation'], cmap='viridis', s=100)\n",
    "plt.colorbar(scatter, label='Elevation (m)')\n",
    "plt.title('Fine Thickness vs Total Depth')\n",
    "plt.xlabel('Total Depth (m)')\n",
    "plt.ylabel('Fine Thickness (m)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('fine_thickness_vs_total_depth.png')\n",
    "plt.close()\n",
    "\n",
    "# Summary statistics for Fine, Medium, Coarse percentages\n",
    "summary_stats = percentage_by_category[['Fine', 'Medium', 'Coarse']].describe()\n",
    "summary_stats = summary_stats.loc[['mean', '50%', 'min', 'max']]\n",
    "\n",
    "# Step 12: Confirm outputs and show sample data\n",
    "print(\"Updated main dataset saved to 'updated_data_final.csv'\")\n",
    "print(\"Percentage of fine, medium, and coarse layers saved to 'fine_percentage.csv'\")\n",
    "print(\"Total thickness of fine layers saved to 'fine_thickness.csv'\")\n",
    "print(\"Bar chart saved to 'layer_percentage_bar.png'\")\n",
    "print(\"Pie charts saved as 'pie_chart_<point_name>.png' for each point\")\n",
    "print(\"Histogram of Fine percentages saved to 'fine_percentage_histogram.png'\")\n",
    "print(\"Scatter plot of Fine Thickness vs Total Depth saved to 'fine_thickness_vs_total_depth.png'\")\n",
    "print(\"\\nColumns in updated dataset:\", df.columns.tolist())\n",
    "print(\"\\nSample of updated main dataset:\")\n",
    "print(df[['point_name', 'Depth_1', 'Depth_2', 'Total_Depth', 'Thickness_meters', 'Thickness_Percent', 'Water_Level_Depth']].head(10))\n",
    "print(\"\\nSample of fine_percentage.csv:\")\n",
    "print(percentage_by_category.head())\n",
    "print(\"\\nSample of fine_thickness.csv:\")\n",
    "print(fine_thickness.head())\n",
    "print(\"\\nSummary statistics for Fine, Medium, Coarse percentages:\")\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4b3af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
